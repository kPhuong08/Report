[{"uri":"https://kphuong08.github.io/Report/5-workshop/5.3-training-flow/5.3.1-lamda-function/","title":"Create Lambda function","tags":[],"description":"","content":" Open the AWS Lambda Console. In the navigation pane, choose Functions, then click Create function. In the Create function console: Name the function: Trigger-Training-Job Runtime: Python 3.14 Permissions: choose Use an existing role and select the role you created earlier. After clicking Create function, the Lambda function will be created.\nBefore using the function, update the role\u0026rsquo;s policy to allow Lambda to assume the role.\nGo to IAM Console → Roles → select the role you created, then edit the Trust relationship and add the statement allowing Lambda and S3to assume the role.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;sagemaker.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;s3.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Once the function is created, paste the following code into the function source:\nimport boto3 import os import time sm = boto3.client(\u0026#39;sagemaker\u0026#39;) def lambda_handler(event, context): # 1. Parse information from the S3 event record = event[\u0026#39;Records\u0026#39;][0] bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # 2. Define a unique job name job_name = f\u0026#34;serverless-mlops-{int(time.time())}\u0026#34; # 3. Configure the Training Job image_uri = \u0026#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\u0026#34; role = os.environ.get(\u0026#39;ROLE_ARN\u0026#39;) # Set this environment variable in the Lambda configuration print(f\u0026#34;Starting training job {job_name} using data from s3://{bucket}/{key}\u0026#34;) sm.create_training_job( TrainingJobName=job_name, HyperParameters={ \u0026#39;sagemaker_program\u0026#39;: \u0026#39;train.py\u0026#39;, \u0026#39;sagemaker_submit_directory\u0026#39;: f\u0026#34;s3://{bucket}/code/source.tar.gz\u0026#34; }, AlgorithmSpecification={ \u0026#39;TrainingImage\u0026#39;: image_uri, \u0026#39;TrainingInputMode\u0026#39;: \u0026#39;File\u0026#39;, }, RoleArn=role, InputDataConfig=[{ \u0026#39;ChannelName\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#34;s3://{bucket}/{key}\u0026#34;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } } }], OutputDataConfig={\u0026#39;S3OutputPath\u0026#39;: f\u0026#34;s3://{bucket}/output/\u0026#34;}, ResourceConfig={\u0026#39;InstanceType\u0026#39;: \u0026#39;ml.m5.large\u0026#39;, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 5}, StoppingCondition={\u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600} ) return {\u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: job_name} Click Deploy to save the function.\nGo to the Configuration tab to add environment variables:\nKey: ROLE_ARN Value: ARN of the role you created earlier Click Save.\n"},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.4-endpoint/5.4.1-model-create/","title":"Create Model","tags":[],"description":"","content":"After the Training Job in section 5.3 completes, a model.tar.gz file will appear in the S3 bucket under the output/ folder.\nThis file is the model artifact. To serve inference requests we must deploy it to a compute environment.\nIn this lab we will use SageMaker Serverless Inference:\nReal-time Endpoint (traditional): runs continuously and incurs costs even when idle. Serverless Endpoint: scales to zero when idle; compute starts on demand and you pay only for actual processing time. Create the model object This step packages model and inference code so SageMaker can load them into a container.\nIn the SageMaker Console, open Deployment \u0026amp; inference in the navigation bar. Under Deployable models, click Create model. Configuration suggestions:\nModel name: model-serverless IAM role: choose the role you created in prerequisites Container definition:\nProvide model artifacts and inference image location Inference image: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3 (Scikit-learn/Python runtime) Model artifacts: link to the model.tar.gz produced by the Training Job (e.g., s3://\u0026lt;your-bucket\u0026gt;/output/\u0026lt;job-id\u0026gt;/model.tar.gz) Environment variables (to point SageMaker to your inference code):\nSAGEMAKER_PROGRAM: train.py SAGEMAKER_SUBMIT_DIRECTORY: S3 path to source.tar.gz After configuring, click Create model. \u0026quot;\n"},{"uri":"https://kphuong08.github.io/Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Report: “Cloud Day VietNam 2025: Ho Chi Minh City Connect Edition for Builders” Event Objectives Sharing trends in AI. AWS solutions applied within enterprises. Insights on data analytics and artificial intelligence to drive development. Migration, modernization, and application building on AWS. This report focuses on the Migration \u0026amp; Modernization content from the event.\nList of Speakers Nguyen Van Hai - Director of Software Engineering, Techcombank \u0026amp; Son Do - Technical Account Manager, AWS Phuc Nguyen - Solutions Architect, AWS \u0026amp; Alex Tran - AI Director, OCB Hung Hoang - Customer Solutions Manager, AWS Taiki Dang - Solutions Architect, AWS Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Nguyen Minh Ngan - AI Specialist, OCB Nguyen The Vinh - Co-Founder \u0026amp; CTO, Ninety Eight Key Highlights Large-scale Migration and Modernization to AWS Addressed enterprise problems and challenges in scaling infrastructure to meet customer needs. Discussed the drivers and processes leading to the decision to move from on-prem to cloud. Outlined the resource migration process -\u0026gt; identifying which resources to move to the cloud first and which to move later. Application Modernization with Generative AI-Powered Tools Explored how Amazon Q Developer transforms the Software Development Life Cycle (SDLC) by acting as an agent across the AWS Console, IDE, CLI, and DevSecOps platforms. Shared how GenAI tools accelerate software development, automate documentation, testing, and virtualization. Panel Discussion: Business Transformation Insights from experts on applying AI in business. Real-world case studies on modernizing applications to accelerate speed and innovation within enterprises. VMware Transformation with AI-driven Cloud Modernization Shared how AWS Transform enables fast, secure, and cost-effective migration from VMware to the cloud. The modernization roadmap to EKS, RDS, and serverless after deployment. AWS Security at Scale: From Development to Production Introduction to AWS Security Hub. Shared methods for enhancing cloud security at scale. How to integrate GenAI into security analysis and operations automation. Lessons Learned Current Technology Transformation Trends GenAI: The robust development of AI and the trend of applying it within enterprises. Migrate \u0026amp; Modernization: The trend of moving and modernizing technology within businesses. Security: Alongside development and transformation, security is essential to protect data, applications, and technology. Approach to Transformation and Modernization Drivers: Based on current and future business problems, workforce, skill levels, and support. Planning \u0026amp; Factors: Establishing a direction suitable for the current enterprise situation with a detailed plan. Modernization Strategy: Having a specific plan regarding what parts to migrate, how to migrate, and when to migrate. GenAI Tools and Applications Learned more about GenAI tools (Amazon Q) and how to apply them to accelerate the application development process. Understood Q Developer\u0026rsquo;s capability to understand complex codebases, suggest optimizations, and automate routine tasks throughout the development lifecycle. Cloud Security Learned about security rules in the cloud, how to integrate security principles from the design stage into the entire development process, and how to use advanced detection and response capabilities. Explored how GenAI enhances security analysis and automates operations. Learned about AWS Security Hub for security management. Application to Work GenAI Tools: Application in supporting development and security. Security: Applying security rules to applications. Event Experience Participating in “Cloud Day VietNam 2025” was a very rewarding experience, helping me learn more about GenAI, security, the Application Migrate \u0026amp; Modernization process, and understand more about real-world case studies from enterprises. Some standout experiences included:\nLearning from High-Expertise Speakers Speakers from AWS and major tech organizations shared the most practical realities of how enterprises are operating. Through real case studies, I gained a clearer understanding of GenAI and Migrate \u0026amp; Modernization in practical applications. Networking and Exchange The event created opportunities to exchange directly with experts and peers with shared passions, helping me gain knowledge and better understand current technology trends. Through practical examples, I learned that GenAI and Migrate \u0026amp; Modernization are the technological directions that companies are aiming for. Key Takeaways GenAI is developing exceptionally fast and is being applied by more and more enterprises into their systems. Migration and modernization strategies need to be evaluated and planned in detail; one should not rush to transform the entire system. AI tools like Amazon Q Developer can support the acceleration of software development. Overall, the event not only provided technical knowledge but also helped me learn more about AI trends and transformation \u0026amp; modernization in the enterprise.\n"},{"uri":"https://kphuong08.github.io/Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Report: “Secure Your Applications: AWS Perimeter Protection Workshop” Event Objectives Sharing insights on CloudFront, WAF, and AWS Shield. Introduction to designing and optimizing global content delivery with CloudFront. Methods to protect web applications from threats using AWS WAF. Hands-on practice on security and web application optimization. List of Speakers Nguyen Gia Hung - Head of Solutions Architect, AWS Julian Ju - Senior Edge Services Specialist Solutions Architect, AWS Key Highlights Introduction to CloudFront Operation: Operates based on a network of Points of Presence (PoPs) to route traffic to the location closest to the user. Connectivity \u0026amp; Protocols: HTTP/3 Multiplexing, TCP Optimization (Persistent Connection, Connection Pooling, TCP Window Scaling), AWS Global Backbone. Origin cloaking: Hiding the origin server (S3 \u0026ldquo;block all\u0026rdquo; - best practice). Access Control: Signed URL/Cookies, Geographic Restriction. Performance: Multi-layer cache, request collapsing, TTL (Time-to-Live). Reliability: CloudFront Origin Failover (Request Level), DNS Failover (Route 53 Global Scale), Logic at Edge. Observability: CloudWatch RUM (Real User Monitoring), Internet Monitor. Security Issues in Applications DDoS attacks on network infrastructure. Application vulnerabilities causing attack risks. Bot traffic (fake clients) depleting resources. Brute-force and flood attacks. Introduction to WAF and Application Protection AWS Shield: Protects layers 3-4, automatically detects and blocks DDoS attacks on network infrastructure. AWS WAF: Protects layer 7, blocks application vulnerabilities (CVE), SQLi, XSS. Bot Control: Prevents bot traffic (fake clients) that causes cost overruns and data skew. Rate limit: Limits the number of requests to prevent brute-force or flood attacks. Lessons Learned AWS CloudFront Understood how CloudFront works and explored enterprise case studies using CloudFront. Learned how to use CloudFront and how to optimize web applications with it. Security with WAF Learned about security risks and vulnerabilities. Understood how WAF operates and how it prevents security attacks. Learned how to use WAF to prevent vulnerabilities and basic attacks. Application to Work Applying CloudFront in Web Deployment: Using CloudFront to optimize Web applications and deploy in a Serverless direction, optimizing costs for simple websites that do not require heavy processing. Applying WAF in Security: Using WAF to secure data flows within the system, allowing trusted data flows to pass while blocking suspicious ones. Event Experience Participating in the “Secure Your Applications: AWS Perimeter Protection Workshop” was a very rewarding experience, helping me gain knowledge about CDNs, optimizing Web application deployment on the internet, and cloud security. Some standout experiences included:\nLearning from High-Expertise Speakers AWS speakers shared best practices in design optimization and application security. Through real-world case studies, I clearly understood how to apply CloudFront and AWS WAF in practical scenarios. Hands-on Technical Experience The workshop included practicing the deployment of a Web application on CloudFront, which helped me understand how CloudFront operates. Practicing security with WAF through various scenarios helped me understand how WAF secures applications and how to deploy security configurations based on specific needs. Networking and Exchange Participating in listening and discussing with speakers as well as fellow attendees helped me connect and learn more about AWS services, specifically CloudFront and WAF. Practical implementation of CloudFront and WAF under the guidance of speakers helped me learn the methods and understand the mechanics of these services better. Key Takeaways CloudFront: A CDN service that assists in optimizing Web application deployment on the Internet. WAF: A security service that helps users prevent security vulnerabilities as well as cyber attacks on applications. Overall, the event not only provided technical knowledge but also helped me learn about services that support efficient, cost-optimized, and more secure Web application deployment.\n"},{"uri":"https://kphuong08.github.io/Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Thai Kieu Phuong\nPhone Number: 0943824030\nEmail: tk.phuong08@gmail.com\nUniversity: University of Information Technology\nMajor: Computer Netwoks and Data Communications\nClass:\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/2025 to 1/2026\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Automating MLOps on AWS using SageMaker with Serverless approach MLOps\nA convergence of Machine Learning, DevOps, and Data Engineering. It constitutes a closed-loop, automated process encompassing stages from Data Ingestion, Training, and Evaluation to Deployment and Monitoring. The primary objective is to minimize manual intervention and accelerate the time-to-market for machine learning models.\nSageMaker AI\nA comprehensive AWS platform designed to build, train, and deploy ML models. In this lab, we will leverage SageMaker\u0026rsquo;s most advanced features:\nSageMaker Training: Utilizes Spot Instances to significantly reduce training costs.\nSageMaker Serverless Inference: Deploys model APIs without the need for server management, featuring automatic scaling to zero when idle (Cost Optimization).\nSageMaker Pipelines (or Step Functions): Used to orchestrate and coordinate the entire workflow.\n"},{"uri":"https://kphuong08.github.io/Report/3-blogstranslated/3.1-blog1/","title":"Migrate MLflow tracking servers to Amazon SageMaker AI with Serverless MLflow","tags":[],"description":"","content":"Migrate MLflow tracking servers to Amazon SageMaker AI with serverless MLflow Link blog : link\nOperating a self-managed MLflow tracking server comes with administrative overhead, including server maintenance and resource scaling. As teams scale their ML experimentation, efficiently managing resources during peak usage and idle periods is a challenge. Organizations running MLflow on Amazon EC2 or on-premises can optimize costs and engineering resources by using Amazon SageMaker AI with serverless MLflow.\nThis post shows you how to migrate your self-managed MLflow tracking server to a MLflow App – a serverless tracking server on SageMaker AI that automatically scales resources based on demand while removing server patching and storage management tasks at no cost. Learn how to use the MLflow Export Import tool to transfer your experiments, runs, models, and other MLflow resources, including instructions to validate your migration’s success.\nWhile this post focuses on migrating from self-managed MLflow tracking servers to SageMaker with MLflow, the MLflow Export Import tool offers broader utility. You can apply the same approach to migrate existing SageMaker managed MLflow tracking servers to the new serverless MLflow capability on SageMaker. The tool also helps with version upgrades and establishing backup routines for disaster recovery.\nStep-by-step guide: Tracking server migration to SageMaker with MLflow The following guide provides step-by-step instructions for migrating an existing MLflow tracking server to SageMaker with MLflow. The migration process consists of three main phases: exporting your MLflow artifacts to intermediate storage, configuring an MLflow App, and importing your artifacts. You can choose to execute the migration process from an EC2 instance, your personal computer, or a SageMaker notebook. Whichever environment you select must maintain connectivity to both your source tracking server and your target tracking server. MLflow Export Import supports exports from both self-managed tracking servers and Amazon SageMaker MLflow tracking servers (from MLflow v2.16 onwards) to Amazon SageMaker Serverless MLflow.\nFigure 1: Migration process with MLflow Export Import tool\nPrerequisites To follow along with this post, make sure you have the following prerequisites:\nAn AWS account—if you don’t have one, sign up as a new customer. Connectivity to both source and target tracking servers (see documentation for self-managed MLflow and MLflow on Amazon SageMaker AI) AWS Identity and Access Management (IAM) permissions to create a SageMaker MLflow App (see Set up IAM permissions for MLflow) An execution environment (EC2, local machine, or SageMaker notebook) with Python 3.10+ installed and adequate storage and compute resources for your tracking server’s data size Execution environment configured with IAM permissions for Serverless MLflow (see SageMaker MLflow IAM requirements) Step 1 — Verify MLflow compatibility Before starting the migration, remember that not all MLflow features may be supported in the migration process. The MLflow Export Import tool supports different objects based on your MLflow version. To prepare for a successful migration:\nVerify the current MLflow version of your existing MLflow tracking server: mlflow --version Review the latest supported MLflow version in the Amazon SageMaker MLflow documentation. If you’re running an older MLflow version in a self-managed environment, we recommend upgrading to the latest version supported by Amazon SageMaker MLflow before proceeding with the migration: pip install --upgrade mlflow=={supported_version} For an up-to-date list of MLflow resources that can be transferred using MLflow Export Import, please refer to the MLflow Export Import documentation. Step 2: Create a new MLflow App o prepare your target environment, you first need to create a new SageMaker Serverless MLflow App.\nAfter you’ve setup SageMaker AI (see also Guide to getting set up with Amazon SageMaker AI), you can access Amazon SageMaker Studio and in the MLflow section, create a new MLflow App (if it wasn’t automatically created during the initial domain setup). Follow the instructions outlined in the SageMaker documentation. Once your managed MLflow App has been created, it should appear in your SageMaker Studio console. Keep in mind that the creation process can take up to 5 minutes. Figure 2: MLflow App in SageMaker Studio Console\nAlternatively, you can view it by executing the following AWS Command Line Interface (CLI) command:\naws sagemaker list-mlflow-tracking-servers Copy the Amazon Resource Name (ARN) of your tracking server to a document, it’s needed in Step 4 Choose Open MLflow, which leads you to an empty MLflow dashboard. In the next steps, we import our experiments and related artifacts from our self-managed MLflow tracking server here. Figure 3: MLflow user interface, landing page\nStep 3: Install MLflow and the SageMaker MLflow plugin To prepare your execution environment for the migration, you need to establish connectivity to your existing MLflow servers (see prerequisites) and install and configure the necessary MLflow packages and plugins.\nBefore you can start with the migration, you need to establish connectivity and authenticate to the environment hosting your existing self-managed MLflow tracking server (e.g., a virtual machine). Once you have access to your tracking server, you need to install MLflow and the SageMaker MLflow plugin in your execution environment. The plugin handles the connection establishment and authentication to your MLflow App. Execute the following command (see also the documentation): pip install mlflow sagemaker-mlflow Step 4: Install the MLflow Export Import tool Before you can export your MLflow resources, you need to install the MLflow Export Import tool.\nFamiliarize yourself with the MLflow Export Import tool and its capabilities by visiting its GitHub page. In the following steps, we make use of its bulk tools (namely export-all and import-all), which allow you to create a copy of your tracking server with its experiments and related artefacts. This approach maintains the referential integrity between objects. If you want to migrate only selected experiments or change the name of existing experiments, you can use Single tools. Please review the MLflow Export Import documentation for more information on supported objects and limitations. Install the MLflow Export Import tool in your environment, by executing the following command: pip install git+https://github.com/mlflow/mlflow-export-import/#egg=mlflow-export-import Step 5: Export MLflow resources to a directory Now that your environment is configured, we can begin the actual migration process by exporting your MLflow resources from your source environment.\nAfter you’ve installed the MLflow Export Import tool, you can create a target directory in your execution environment as a destination target for the resources, which you extract in the next step. Inspect your existing experiments and the associated MLflow resources you want to export. In the following example, we want to export the currently stored objects (for example, experiments and registered models). Figure 4: Experiments stored in MLflow\nStart the migration by configuring the Uniform Resource Identifier (URI) of your tracking server as an environmental variable and executing the following bulk export tool with the parameters of your existing MLflow tracking server and a target directory (see also the documentation): # Set the tracking URI to your self-managed MLflow server export MLFLOW_TRACKING_URI=http://localhost:8080 # Start export export-all --output-dir mlflow-export Wait until the export has finished to inspect the output directory (in the preceding case: mlflow-export). Step 6: Import MLflow resources to your MLflow App During import, user-defined attributes are retained, but system-generated tags (e.g., creation_date) are not preserved by MLflow Export Import. To preserve original system attributes, use the \u0026ndash;import-source-tags option as shown in the following example. This saves them as tags with the mlflow_exim prefix. For more information, see MLflow Export Import – Governance and Lineage. Be aware of additional limitations detailed here: Import Limitations.\nThe following procedure transfers your exported MLflow resources into your new MLflow App:Start the import by configuring the URI for your MLflow App. You can use the ARN–which you saved in Step 1–for this. The previously installed SageMaker MLflow plugin automatically translates the ARN in a valid URI and creates an authenticated request to AWS (remember to configure your AWS credentials as environmental variables so the plugin can pick them up).\n# Set the tracking URI to your MLflow App ARN export MLFLOW_TRACKING_URI=arn:aws:sagemaker:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:mlflow-app/app-\u0026lt;app-id\u0026gt; # Start import import-all --input-dir mlflow-export Step 7: Validate your migration results To confirm your migration was successful, verify that your MLflow resources were transferred correctly:\nOnce the import-all script has migrated your experiments, runs, and other objects to the new tracking server, you can start verifying the success of the migration, by opening the dashboard of your serverless MLflow App (which you opened in Step 2) and verify that: Exported MLflow resources are present with their original names and metadata Run histories are complete with the metrics and parameters Model artifacts are accessible and downloadable Tags and notes are preserved Figure 5: MLflow user interface, landing page after migration\nYou can verify programmatic access by starting a new SageMaker notebook and running the following code: import mlflow # Set the tracking URI to your MLflow App ARN mlflow.set_tracking_uri(\u0026#39;arn:aws:sagemaker:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:mlflow-app/app-\u0026lt;app-id\u0026gt;\u0026#39;) # List all experiments experiments = mlflow.search_experiments() for exp in experiments: print(f\u0026#34;Experiment Name: {exp.name}\u0026#34;) # Get all runs for this experiment runs = mlflow.search_runs(exp.experiment_id) print(f\u0026#34;Number of runs: {len(runs)}\u0026#34;) Considerations When planning your MLflow migration, verify your execution environment (whether EC2, local machine, or SageMaker notebooks) has sufficient storage and computing resources to handle your source tracking server’s data volume. While the migration can run in various environments, performance may vary based on network connectivity and available resources. For large-scale migrations, consider breaking down the process into smaller batches (for example, individual experiments).\nCleanup A SageMaker managed MLflow tracking server will incur costs until you delete or stop it. Billing for tracking servers is based on the duration the servers have been running, the size selected, and the amount of data logged to the tracking servers. You can stop tracking servers when they’re not in use to save costs, or you can delete them using API or the SageMaker Studio UI. For more details on pricing, refer to Amazon SageMaker pricing.\nConclusion In this post, we demonstrated how to migrate a self-managed MLflow tracking server to SageMaker with MLflow using the open source MLflow Export Import tool. The migration to a serverless MLflow App on Amazon SageMaker AI reduces the operational overhead associated with maintaining MLflow infrastructure while providing seamless integration with the comprehensive AI/ML serves in SageMaker AI.\nTo get started with your own migration, follow the preceding step-by-step guide and consult the referenced documentation for additional details. You can find code samples and examples in our AWS Samples GitHub repository. For more information about Amazon SageMaker AI capabilities and other MLOps features, visit the Amazon SageMaker AI documentation.\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Join the community, initially get acquainted with members of First Cloud Journey. Create an AWS account, learn the basics of using the Console and CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members 1/9/2025 11/08/2025 3 - Learn about AWS and service types 2/09/2025 2/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 3/09/2025 3/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Create Budget and IAM + Configure MFA 4/09/2025 4/09/2025 6 - Learn basics of simple services: EC2, VPC, S3,\u0026hellip; 5/09/2025 5/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Connected and got acquainted with members in the community.\nSuccessfully created and configured an AWS Free Tier account.\nCreated Budget for account management, MFA for security, and IAM.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer including:\nAccess Key Secret Key Region Used AWS CLI to perform basic operations such as:\nChecking account info \u0026amp; configuration Getting the list of services Combined web interface and CLI to manage AWS resources in parallel.\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn AWS services Understand how services operate and their data flows Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study AWS architecture and components: + Compute + Storage + Networking + Database 8/09/2025 8/09/2025 3 - Learn EC2: + How to create an instance + How to SSH + Instance types + Practice: + Create EC2 + SSH into EC2 + Launch EC2 with user data 9/09/2025 9/09/2025 https://000004.awsstudygroup.com/ 4 - Learn VPC: + Region, AZ + Subnet, route table, security group + Internet gateway, NAT gateway + Practice: + Create subnet, route table + Create security group + Create internet gateway, NAT gateway + Build a complete VPC with private and public subnets and connect internet-public-private 10/09/2025 10/09/2025 https://000003.awsstudygroup.com/ 5 - Learn database types: + S3 + RDS + DynamoDB - Learn Elastic IP 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create a VPC and an EC2 instance + Connect via SSH + Run a simple static web and test access 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood AWS components:\nCompute Storage Networking Database Became familiar with creating VPCs and EC2 instances\nLearned about AWS networking (security groups, route tables, NAT, internet gateway)\nGained knowledge about AWS storage and databases\nCreated and hosted a basic static website using EC2 and confirmed external access\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn and practice AWS services Explore AI services for the project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get familiar with AWS AI services: + AI overview in AWS + Intro to Bedrock + Intro to Nova + Intro to Amazon Q + Intro to SageMaker 15/09/2025 15/09/2025 3 - Study service basics: + Types and support + Architecture + Pricing 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Read and learn about MLOps and how AWS supports MLOps 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn databases: + S3, RDS, DynamoDB - Practice: + Create, delete, update + Connect database to a simple website 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn EC2 ABL and auto scaling Practice: + Launch EC2 instance + Connect to EC2 via ABL + Create auto scaling for EC2 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Learned an overview of AWS AI services:\nBedrock Nova Amazon Q SageMaker Studied AWS databases and practiced creating databases\nConnected databases to EC2 for a simple website\nExplored ABL and auto scaling for EC2\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn additional AWS services Decide which AI service to use for the project Plan the project work Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Summarize features, pricing, and capabilities of the AI services researched - Decide which service and how to use/implement it 22/09/2025 22/09/2025 3 - Learn the basics of Serverless and Microservices 23/09/2025 23/09/2025 4 - Study Lambda: + How to use + Execution flow 24/09/2025 24/09/2025 https://000022.awsstudygroup.com/ 5 - Practice using Lambda: + Create function + Create trigger 25/09/2025 25/09/2025 https://000022.awsstudygroup.com/ 6 - Draft project plan: + Services to research + Components to implement 26/09/2025 26/09/2025 Week 4 Achievements: Improved understanding of AI services\nChosen the AI service to use for the project\nGained a basic understanding of Serverless and Microservices\nLearned about Lambda and how to create functions\nDrafted a project plan:\nServices to research Components to implement "},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn how to use SageMaker AI Explore related AWS services Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get familiar with SageMaker AI: + Architecture + Execution flow + How services integrate in SageMaker + Serverless patterns in SageMaker 29/09/2025 29/09/2025 3 - Learn CloudWatch - Practice: create a CloudWatch alarm 30/09/2025 30/09/2025 https://000008.awsstudygroup.com/ 4 - Learn EventBridge - Practice: create an EventBridge trigger to schedule a Lambda invocation 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Explore SageMaker Studio and JumpStart, and deploy a model from JumpStart 02/10/2025 02/10/2025 6 - Learn about GitHub Actions 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Gained knowledge about SageMaker AI:\nSageMaker Studio JumpStart Deployed a model in JumpStart (trial)\nLearned CloudWatch and used it to monitor logs\nLearned EventBridge and used it to trigger events\nStudied GitHub Actions for CI/CD integration\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Build project architecture and data flows between services Learn how to connect services together Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study flows between services - How to connect Lambda to SageMaker - How to use EventBridge to trigger Lambda 06/10/2025 06/10/2025 3 - Build individual data flows - Design architecture for each flow 07/10/2025 07/10/2025 4 - Study how to deploy serverless endpoints in SageMaker 08/10/2025 08/10/2025 5 - Learn GitHub Actions integration with AWS 09/10/2025 09/10/2025 6 - Build the data flow from GitHub to AWS 10/10/2025 10/10/2025 Week 6 Achievements: Gained deeper understanding of flows between services in AWS\nLearned how GitHub Actions integrates with AWS\nUnderstood how to deploy serverless endpoints in SageMaker\nBuilt a basic architecture and data flow\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Build infrastructure using the AWS Console Deploy workflows manually Tune services and parameters as needed Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create an S3 bucket and EventBridge rule - Trigger S3 upload event with EventBridge 13/10/2025 13/10/2025 3 - Try manual model and endpoint deployment on SageMaker - Create a Lambda function to automate deployment 12/08/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Prepare data and model for manual training jobs on SageMaker 15/10/2025 15/10/2025 5 - Create Git workflow and test trigger with git 16/10/2025 16/10/2025 6 - Tune model and data flow between services 17/10/2025 17/10/2025 Week 7 Achievements: Deployed infrastructure manually through the Console\nImproved understanding of service behaviors\nImplemented manual workflows between services\nCreated a Git-based trigger workflow\nTuned system model and data flows\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Build infrastructure using Terraform Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - List required modules and plan folder structure 20/10/2025 20/10/2025 3 - Write modules + S3 + EventBridge 21/10/2025 21/10/2025 4 - Write modules + CloudWatch + IAM 22/10/2025 22/10/2025 5 - Deploy and fix bugs 23/10/2025 15/08/2025 https://cloudjourney.awsstudygroup.com/ 6 - Write lambda trigger training module and deploy 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Created project folder structure\nWrote core infrastructure modules\nS3 IAM Cloudwatch Eventbridge Lambda Deployed and found bugs\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Implement infrastructure modules Write lambda functions Prepare model and data scripts Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Write lambda training trigger script 27/10/2025 27/10/2025 3 - Write lambda redeploy function 28/10/2025 28/10/2025 4 - Deploy and fix 29/10/2025 29/10/2025 5 - Write scripts to prepare model and data 30/10/2025 30/10/2025 6 - Write git workflows to test deploy and fix bugs 31/10/2025 31/10/2025 Week 9 Achievements: Implemented lambda modules Wrote 2 lambda functions Deployed and found multiple bugs "},{"uri":"https://kphuong08.github.io/Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learn AWS services\nWeek 3: Learn and research for the project\nWeek 4: Plan for the project and test\nWeek 5: Learn related services and try to use them\nWeek 6: Build architecture and flow\nWeek 7: Test services and adjust\nWeek 8: Write basic infrastructure\nWeek 9: Write script and function\nWeek 10: Deploy and test\nWeek 11: Review\nWeek 12: Write report\n"},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.3-training-flow/5.3.2-s3-event-trigger/","title":"Configure S3 Event Trigger","tags":[],"description":"","content":"Configure S3 Event Trigger Open your Lambda function in the console. Choose Add trigger and select S3 as the source. In Trigger configuration:\nBucket: choose the bucket you created earlier Event type: PUT Prefix: data/train/ (only trigger on uploads to this folder) Suffix: .csv (only trigger for CSV files) Check the acknowledgement box and click Add.\nTest the workflow Create a test file test-train.csv locally. Upload the file to the S3 path /data/train/ using the S3 console or CLI. After a successful upload:\nOpen SageMaker Studio / Console → Model training \u0026amp; customization → Training \u0026amp; tuning jobs. You should see a new Training Job in InProgress state. This confirms the Event-Driven workflow triggered the training job successfully. "},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.4-endpoint/5.4.2-serverless-endpoint/","title":"Configure Serverless Endpoint","tags":[],"description":"","content":"Create Endpoint Configuration In the SageMaker Console, open Deployment \u0026amp; inference. Under Endpoint configurations, click Create endpoint configuration. Configuration options:\nName: serverless-endpoint Type of endpoint: Serverless Click Create production variant and select the model you created in the previous step.\nPress Save.\nEdit the Variant settings as needed:\nName: variant-serverless Max concurrency: 10 (default; request an increase from AWS if you need higher concurrency) Click Create endpoint configuration.\nCreate Endpoint In the SageMaker Console, open Deployment \u0026amp; inference. Under Endpoints, click Create endpoint. Configuration options:\nName: endpoint-deploy-serverless Attach endpoint configuration: Use an existing endpoint configuration (select the one you just created) Click Create endpoint to deploy the Serverless endpoint.\n"},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Create an IAM Role named SageMakerExecutionRole and attach the following policies (at minimum):\nAmazonSageMakerFullAccess AmazonS3FullAccess CloudWatchLogsFullAccess AWSLambda_FullAccess Save the Role ARN; you\u0026rsquo;ll need it in later steps.\nCreate an S3 Bucket First, we need a place to store training data and model artifacts.\nCreate the bucket and enable versioning (leave other fields as default).\nCreate the following folders (you can create empty objects to ensure the folders exist):\ndata/train/ – where you upload files that will trigger training data/test/ – test data models/ – where SageMaker will store trained model artifacts code/ – training code (e.g. train.py) Training script \u0026amp; sample data Prepare a simple training script locally to simulate training.\nExample train.py:\nimport argparse import os import time import joblib import shutil import json # --------------------------------------------------------- # PHẦN 1: CÁC HÀM INFERENCE # --------------------------------------------------------- def model_fn(model_dir): \u0026#34;\u0026#34;\u0026#34;Load model từ ổ cứng\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Đang load model từ: {model_dir}\u0026#34;) model_path = os.path.join(model_dir, \u0026#39;model.joblib\u0026#39;) if os.path.exists(model_path): return joblib.load(model_path) else: raise FileNotFoundError(f\u0026#34;Không tìm thấy file model tại: {model_path}\u0026#34;) def input_fn(request_body, request_content_type): \u0026#34;\u0026#34;\u0026#34; Hàm này nhận Raw Bytes từ request và chuyển thành object Python \u0026#34;\u0026#34;\u0026#34; if request_content_type == \u0026#39;application/json\u0026#39;: # Chuyển chuỗi JSON (bytes) thành Python List/Dict return json.loads(request_body) else: # Nếu gửi content-type khác, báo lỗi raise ValueError(f\u0026#34;Content type {request_content_type} chưa được hỗ trợ.\u0026#34;) def predict_fn(input_data, model): \u0026#34;\u0026#34;\u0026#34; Logic dự đoán \u0026#34;\u0026#34;\u0026#34; # Vì đây là dummy model (Dict), ta giả lập việc dự đoán # input_data lúc này đã là List/Dict nhờ hàm input_fn ở trên result = { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;prediction_from_model\u0026#34;: model, # Model dummy của bạn \u0026#34;input_you_sent\u0026#34;: input_data } return result def output_fn(prediction, content_type): \u0026#34;\u0026#34;\u0026#34; Hàm này nhận kết quả từ predict_fn và chuyển thành JSON để trả về Client. QUAN TRỌNG: Khắc phục lỗi 500 tại đây. \u0026#34;\u0026#34;\u0026#34; # Mặc định SageMaker mong đợi accept header là application/json if content_type == \u0026#39;application/json\u0026#39;: return json.dumps(prediction) raise ValueError(f\u0026#34;Accept type {content_type} chưa được hỗ trợ.\u0026#34;) # --------------------------------------------------------- # PHẦN 2: CODE TRAINING # --------------------------------------------------------- if __name__ == \u0026#39;__main__\u0026#39;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--train\u0026#39;, type=str, default=os.environ.get(\u0026#39;SM_CHANNEL_TRAIN\u0026#39;)) parser.add_argument(\u0026#39;--model-dir\u0026#39;, type=str, default=os.environ.get(\u0026#39;SM_MODEL_DIR\u0026#39;)) args, _ = parser.parse_known_args() print(\u0026#34;Starting training...\u0026#34;) # time.sleep(5) dummy_model = {\u0026#34;name\u0026#34;: \u0026#34;Dummy Model\u0026#34;, \u0026#34;accuracy\u0026#34;: 99.9} save_path = os.path.join(args.model_dir, \u0026#39;model.joblib\u0026#39;) joblib.dump(dummy_model, save_path) print(f\u0026#34;Training complete. Model saved to {save_path}\u0026#34;) # Lấy đường dẫn file code đang chạy hiện tại current_script_path = __file__ # Định nghĩa đường dẫn đích trong folder output code_output_path = os.path.join(args.model_dir, \u0026#39;train.py\u0026#39;) print(f\u0026#34;Copying code from {current_script_path} to {code_output_path}\u0026#34;) shutil.copy2(current_script_path, code_output_path) # ========================================================= print(\u0026#34;Training and packaging complete.\u0026#34;) Package the code into a tarball:\ntar -czvf source.tar.gz train.py Upload source.tar.gz to the S3 code/ folder.\n"},{"uri":"https://kphuong08.github.io/Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Automate MLOps Automated Fine-tuning and Serverless Deployment Pipeline on AWS 1. Executive Summary The Serverless MLOps Pipeline project is designed to address the challenges of cost optimization and the automation of Machine Learning model training (fine-tuning) and deployment. The system focuses on eliminating manual toil for data engineers by shifting from traditional infrastructure management to an Event-driven and Serverless architecture on AWS.\nThis MLOps workflow allows for automatic re-training triggers when new data arrives in S3, performs automated evaluation, and deploys the model to SageMaker Serverless Inference. The solution minimizes operational costs by leveraging EC2 Spot Instances for training and the \u0026ldquo;scale-to-zero\u0026rdquo; capability of Serverless Endpoints for inference.\n2. Problem Statement What’s the Problem? The current process for developing and operating ML models faces several obstacles:\nManual \u0026amp; Disjointed: Engineers must manually run commands to train, evaluate, and deploy. The code base is difficult to maintain. Resource Waste: Utilizes expensive on-demand instances often left running after training completes. Endpoints run 24/7 regardless of traffic. Lack of Quality Control: Evaluation logic is rudimentary or mocked, leading to risks when deploying poor-quality models to production. The Solution Construct a complete CI/CD/CT (Continuous Training) pipeline:\nTraining: Use SageMaker Training Jobs with Spot Instances to reduce training costs (50-80% savings). Evaluation: Integrate real evaluation scripts that compare metrics (Accuracy, F1) against thresholds before accepting a model. Inference: Transition to SageMaker Serverless Endpoints, which only charge per request and are free when idle. Automation: Use Amazon EventBridge and GitHub Actions to orchestrate the workflow from data ingestion to deployment. Benefits and Return on Investment The solution delivers clear economic efficiency and frees up AI/ML engineers. Estimated operational costs are only around $3.30/month for a standard scenario (compared to tens of dollars with static infrastructure). The time-to-market from new data to an updated model is reduced from days to hours, fully automated. CloudWatch Monitoring ensures immediate detection of training or inference issues.\n3. Solution Architecture The platform applies an Event-driven Serverless architecture to manage the Machine Learning model lifecycle. Training data (CSV/JSON) uploaded to S3 triggers an automated event chain involving training, evaluation, and deployment.\nAWS Services Used Amazon S3: Stores Datasets, Model Artifacts (model.tar.gz), and Training Metrics. Amazon SageMaker: Training Jobs: Runs Fine-tuning with HuggingFace Trainer (using Spot Instances). Serverless Inference: Hosts model endpoints with auto-scaling capabilities (cold start 1-3s). AWS Lambda: Serverless function that executes the endpoint redeployment logic when a new model is available. Amazon EventBridge: Captures state change events (S3 upload, Training success) to orchestrate the flow. Amazon CloudWatch: Dashboard for monitoring training metrics, endpoint latency, and error rates. Workflow Ingest: User uploads the prepared dataset to Amazon S3. Trigger: An S3 Event triggers the process (via EventBridge or Webhook to GitHub Actions). Train: GitHub Actions initiates a SageMaker Training Job (using a HuggingFace container on a Spot Instance). Evaluate: Upon training completion, the pipeline runs a script to download the model and check metrics. If it meets the threshold, the model is saved as a candidate. Deploy: A \u0026ldquo;New Model\u0026rdquo; event triggers AWS Lambda, which updates the SageMaker Serverless Endpoint. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Tests: Research related services and manual test on console Calculate Price and Check Practicality: Estimate costs and check that services are support what we need or not. Fix Architecture for Cost or Solution Fit: Adjust and change (model, services,..) to stay cost-effective and usable. Develop, Test, and Deploy: Configure, write scripts, build infrastructure. Clean \u0026amp; Refactor: Cleanup legacy code, standardize directory structure (src, ops, lambda). Data Preparation Script: Write prepare_data.py to split train/test sets and upload to S3. Training Implementation: Configure train.py using HuggingFace Trainer, integrating metrics.json saving. Evaluation Logic: Write evaluate_model.py to parse metrics and decide Pass/Fail based on thresholds. CI/CD Integration: Configure GitHub Actions workflows to connect the above steps. Serverless Deployment: Write the redeploy_endpoint Lambda function using Boto3 to update the SageMaker Endpoint. Infrastructure as Code: Use Terraform to provision AWS resources (S3, IAM Roles, Lambda). Technical Requirements\nModel: HuggingFace models (e.g., DistilBERT) for NLP tasks. Compute: Training uses ml.g4dn.xlarge (Spot); Inference uses Serverless memory config (1024MB-3072MB). Latency: Acceptable Cold Start of 1-3s for the first request after idle time. 5. Timeline \u0026amp; Milestones Project Timeline\nMonth 1: Study AWS and research. Month 2: Test and adjust. Month 3: Implement, test, and launch. Post-Launch: Research for the development direction. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nInfrastructure Costs AWS Services: Training (SageMaker Spot):~$3/month. Inference (SageMaker Serverless): $3/month. Lưu trữ (Amazon S3): ~$0.3/month. CloudWatch: ~$5/tháng Orchestration (Lambda + EventBridge): ~$0.01/month (likely free in Free Tier). Total: ~$11 USD / month.\n7. Risk Assessment Risk Matrix Spot Instance Interruption: Training interrupted mid-way due to AWS reclaiming the instance. Mitigation: Implement Checkpointing in the training code to resume from the last saved state instead of restarting. Cold Start Latency: The first request takes 1-3 seconds, affecting strict real-time user experiences. Mitigation: Acceptable for internal tasks or non-critical real-time (sub-millisecond) requirements. Provisioned Concurrency can be used if necessary (higher cost). Model Drift: The newly trained model performs worse than the existing one. Mitigation: The evaluate_model.py script acts as a gatekeeper, preventing deployment if metrics are lower than the current version. 8. Expected Outcomes 100% Automation: Zero manual intervention from data ingestion to live model. Cost Optimization: Reduces infrastructure costs to the absolute minimum for an ML project. Scalability: The architecture is easily adaptable to other ML problems (Computer Vision, Tabular data) by simply swapping the training script. Full Monitoring: comprehensive Dashboards regarding the health of training and serving systems. "},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Debug and fix infra code, scripts, and functions Deploy infrastructure to AWS and run tests Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Fix bugs, adjust scripts and functions 03/11/2025 03/11/2025 3 - Fix infra code issues 04/11/2025 04/11/2025 4 - Deploy models to AWS and verify test cases 05/11/2025 05/11/2025 5 - Refactor and fix minor issues 06/11/2025 06/11/2025 6 - Fix bugs and adjust infra code and scripts 07/11/2025 07/11/2025 Week 10 Achievements: Debugged infra code and scripts Fixed code issues Deployed model to AWS and verified tests "},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Wrap up the project Write part of the project report Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Wrap up and complete the project - Propose future directions for the project 10/11/2025 10/11/2025 3 - Write event summaries 11/11/2025 11/11/2025 4 - Write worklogs for weeks 1-4 12/11/2025 12/11/2025 5 - Write worklogs for weeks 5-8 13/11/2025 13/11/2025 6 - Write worklogs for weeks 9-12 14/11/2025 14/11/2025 Week 11 Achievements: Wrapped up the project and proposed future directions\nWrote a portion of the project report\n"},{"uri":"https://kphuong08.github.io/Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete the final report Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Write the project proposal 17/11/2025 17/11/2025 3 - Write the workshop summary and outline 18/11/2025 18/11/2025 4 - Write detailed workshop content 19/11/2025 19/11/2025 5 - Write blog post 20/11/2025 20/11/2025 6 - Write evaluations and feedback 21/11/2025 21/11/2025 Week 12 Achievements: Completed the final report "},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.4-endpoint/5.4.3-deploy-invoke/","title":"Automated deployment and testing","tags":[],"description":"","content":"The previous section covered how to deploy a model and an endpoint on SageMaker manually. This section shows how to automatically deploy a new model as soon as it is uploaded to S3.\nAutomatic flow to create an endpoint and deploy a model Create a Lambda function Click Create function Name: Trigger-Deployment Runtime: Python 3.14 Permission: use an existing role Select the IAM role created in earlier steps Click Create function Create a trigger Click Add trigger Source trigger: S3 Bucket: the bucket that will contain the model Event type: PUT Prefix: output/ Suffix: .tar.gz Click Add Lambda code Add the following code to the Lambda function (adjust environment variables and ARNs to your environment):\nimport boto3 import time import os import logging logger = logging.getLogger() logger.setLevel(logging.INFO) sm_client = boto3.client(\u0026#39;sagemaker\u0026#39;) # CONFIG (recommended to put in Lambda Environment Variables) # Container image URI for Scikit-Learn (change according to your region) # You can get this with: sagemaker.image_uris.retrieve(\u0026#34;sklearn\u0026#34;, region=\u0026#34;us-east-1\u0026#34;, version=\u0026#34;0.23-1\u0026#34;) CONTAINER_IMAGE_URI = \u0026#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\u0026#34; ROLE_ARN = \u0026#34;arn:aws:iam::064197589739:role/SageMakerExecutionRole\u0026#34; # Replace with your role ARN def lambda_handler(event, context): try: # 1. Retrieve the uploaded file info from the S3 event s3_record = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;] bucket = s3_record[\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;].strip() key = s3_record[\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] model_url = f\u0026#34;s3://{bucket}/{key}\u0026#34; # Create unique names based on timestamp timestamp = time.strftime(\u0026#39;%Y-%m-%d-%H-%M-%S\u0026#39;, time.gmtime()) model_name = f\u0026#34;sklearn-model-{timestamp}\u0026#34; endpoint_config_name = f\u0026#34;endpoint-config-{timestamp}\u0026#34; endpoint_name = f\u0026#34;endpoint-serverless-{timestamp}\u0026#34; logger.info(f\u0026#34;Detected new model: {model_url}\u0026#34;) # 2. Create SageMaker Model create_model_response = sm_client.create_model( ModelName=model_name, PrimaryContainer={ \u0026#39;Image\u0026#39;: CONTAINER_IMAGE_URI, \u0026#39;ModelDataUrl\u0026#39;: model_url, \u0026#39;Environment\u0026#39;: { \u0026#39;SAGEMAKER_PROGRAM\u0026#39;: \u0026#39;train.py\u0026#39;, \u0026#39;SAGEMAKER_SUBMIT_DIRECTORY\u0026#39;: model_url } }, ExecutionRoleArn=ROLE_ARN ) logger.info(f\u0026#34;Created Model: {model_name}\u0026#34;) # 3. Create Endpoint Configuration (Serverless) create_config_response = sm_client.create_endpoint_config( EndpointConfigName=endpoint_config_name, ProductionVariants=[ { \u0026#39;VariantName\u0026#39;: \u0026#39;AllTraffic\u0026#39;, \u0026#39;ModelName\u0026#39;: model_name, \u0026#39;ServerlessConfig\u0026#39;: { \u0026#39;MemorySizeInMB\u0026#39;: 2048, # choose 1024, 2048, 3072... \u0026#39;MaxConcurrency\u0026#39;: 5 # max concurrent requests } } ] ) logger.info(f\u0026#34;Created Serverless Config: {endpoint_config_name}\u0026#34;) # 4. Create Endpoint create_endpoint_response = sm_client.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Deploying Endpoint: {endpoint_name}. This may take several minutes.\u0026#34; } except Exception as e: logger.error(f\u0026#34;Error: {e}\u0026#34;) raise e Click Deploy When training and tuning produce a model artifact in S3, this Lambda will automatically deploy the model and create the corresponding endpoint.\nTest endpoint Create input.json with any test data [\u0026#34;testdata\u0026#34;, \u0026#34;test\u0026#34;] Run the following command to invoke the endpoint: Replace the endpoint-name\naws sagemaker-runtime invoke-endpoint --endpoint-name endpoint-serverless-2026-01-09-06-04-49 --body fileb://input.json --content-type application/json output_file.json Response will be save in output_file.json: { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;prediction\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Dummy Model\u0026#34;, \u0026#34;accuracy\u0026#34;: 99.9 }, \u0026#34;input_received\u0026#34;: [\u0026#34;testdata\u0026#34;, \u0026#34;test\u0026#34;] } "},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.3-training-flow/","title":"Build automated Training pipeline","tags":[],"description":"","content":"Building an automated Training pipeline In this section, we will set up an event-driven mechanism. The goal is that when new training data is uploaded to an S3 bucket, a Lambda function will be automatically triggered. This Lambda acts as an \u0026ldquo;Orchestrator\u0026rdquo; to call SageMaker APIs and start a new Training Job without human intervention.\nContents Create Lambda function Configure S3 Event \u0026amp; test "},{"uri":"https://kphuong08.github.io/Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Migrate MLflow tracking servers to Amazon SageMaker AI with serverless MLflow This blog post provides a technical guide to migrating a self-managed MLflow monitoring server (on EC2 or on-premises) to Amazon SageMaker using Serverless MLflow. It simplifies complex operations, optimizes costs, and integrates with the SageMaker AI/ML ecosystem. It utilizes the MLflow Import function and the SageMaker MLflow Plugin.\n"},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.4-endpoint/","title":"Deploy Serverless Endpoint","tags":[],"description":"","content":"Deploying a Serverless Endpoint In this section, we will package the model artifact produced by training and deploy it to a SageMaker Serverless Inference endpoint. Unlike a real-time endpoint (which runs continuously), a Serverless Endpoint can scale to zero when idle, helping optimize cost for infrequently used workloads.\nContents Create Model Configure Serverless Endpoint Deploy and Invoke "},{"uri":"https://kphuong08.github.io/Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day VietNam 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Secure Your Applications: AWS Perimeter Protection Workshop\nDate \u0026amp; Time: 09:00, November 19, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://kphuong08.github.io/Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Automating MLOps workflows on AWS using SageMaker (Serverless approach) Overview Serverless MLOps is a modern approach that reduces infrastructure management overhead and optimizes costs by leveraging AWS serverless and auto-scaling services.\nIn this workshop, we will build a closed-loop pipeline: from ingesting raw data into the cloud to preparing a model ready to serve users — all triggered automatically without maintaining continuously-running servers.\nWe will focus on the following Serverless components:\nSageMaker Serverless Inference - The main feature in this lab. It enables deploying an ML model as an API endpoint that can scale to zero when idle, so you pay only for compute time used. AWS Step Functions - Acts as the orchestrator, replacing traditional Jenkins/GitLab Runner servers to manage training and deployment workflows. Amazon EventBridge - Event-driven triggers to detect new data on S3 and start retraining workflows automatically. Contents Workshop overview Prerequisites Build automated training pipeline Deploy Serverless Endpoint Cleanup resources "},{"uri":"https://kphuong08.github.io/Report/5-workshop/5.5-cleanup/","title":"Clean up resources","tags":[],"description":"","content":"Clean up resources This is the most important step to ensure you do not incur ongoing AWS charges after the workshop. We remove resources in the reverse order from creation: Endpoint -\u0026gt; Model -\u0026gt; Data.\nDelete SageMaker resources Replace the endpoint name with your own\nDelete the Endpoint aws sagemaker delete-endpoint --endpoint-name demo-serverless-endpoint # Or the endpoint name created by CLI aws sagemaker delete-endpoint --endpoint-name demo-serverless-endpoint Delete the Endpoint Configuration aws sagemaker delete-endpoint-config --endpoint-config-name demo-serverless-config # Also delete any console-created configs if present aws sagemaker delete-endpoint-config --endpoint-config-name demo-serverless-config Delete the Model # List models to copy the exact name aws sagemaker list-models # Delete the model aws sagemaker delete-model --model-name demo-serverless-model Delete Data and Lambda Clean the S3 bucket # Replace with your bucket name export BUCKET_NAME=\u0026#34;your-bucket-name\u0026#34; # Remove all objects (code, data, model artifacts) aws s3 rm s3://$BUCKET_NAME --recursive # Remove the bucket aws s3 rb s3://$BUCKET_NAME Delete Lambda functions Go to the Lambda console Select the function to delete Choose Action -\u0026gt; Delete Delete CloudWatch Log Groups Go to CloudWatch -\u0026gt; Logs -\u0026gt; Log Management Select log groups to delete -\u0026gt; Action -\u0026gt; Delete log group(s) "},{"uri":"https://kphuong08.github.io/Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Automate MLOps on AWS in Serverless way, through which I improved my skills in programming, analysis, reporting, using AWS.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Improvement in organizing tasks and managing time effectively Improved communication skills in daily and work interactions, as well as within teams Improvement in problem-solving thinking "},{"uri":"https://kphuong08.github.io/Report/7-feedback/","title":"Sharing &amp; Feedback","tags":[],"description":"","content":"General Evaluation 1. Working environment The working environment is friendly and open. FCJ team members are proactive and always ready to support me when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable. I suggest occasional social activities or shared lunches to improve team bonding.\n2. Mentor / Admin support Mentors provide timely guidance, explain unclear points, and encourage questions. The admin team helps with documents and onboarding, enabling smooth work.\n3. Fit with academic background The assigned tasks align well with my university studies while introducing new areas I had not previously encountered, allowing me to strengthen fundamentals and gain practical skills.\n4. Learning \u0026amp; development I learned tools for project management, teamwork practices, and professional communication. Mentors shared real-world experience that helped me plan my career path.\n5. Company culture \u0026amp; teamwork The culture is positive: people respect each other and work hard while maintaining a friendly atmosphere. During tight deadlines, everyone collaborates to support the team.\n6. Internship policies \u0026amp; benefits The company offers flexible working hours and learning opportunities through workshops and events.\nAdditional questions What did you find most satisfying during the internship? Everyone is very friendly, always ready to support Suggestions \u0026amp; requests Would you like to continue in this program in the future? Yes "},{"uri":"https://kphuong08.github.io/Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://kphuong08.github.io/Report/tags/","title":"Tags","tags":[],"description":"","content":""}]